"""
LangGraph Reactive Agent for FortuneDiary chat.

Uses `create_react_agent` (prebuilt ReAct loop) with:
- Google Gemini as LLM
- AsyncPostgresSaver for conversation memory
- Diary RAG search as first tool
"""
from __future__ import annotations

import logging
from typing import Optional, Annotated

from typing import Literal

from langchain_core.tools import tool
from langchain_core.messages import AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import create_react_agent, ToolNode, InjectedState
from psycopg_pool import AsyncConnectionPool

from copilotkit import CopilotKitState
from copilotkit.langgraph import copilotkit_emit_state

from app.config import (
    SUPABASE_DB_URI, GOOGLE_API_KEY, DEFAULT_CHAT_MODEL,
    THINKING_ENABLED, THINKING_LEVEL,
)
from app.agent.prompts import build_system_prompt, build_diary_system_prompt, load_fortune_context
from app.services.vector_service import vector_service
from app.services.bazi_service import bazi_service
from app.services.tarot_service import tarot_service
from app.core.db import supabase

logger = logging.getLogger(__name__)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Tools (defined here so they can read RunnableConfig for user_id)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@tool
async def search_diaries(query: str, config: RunnableConfig, max_results: int = 5) -> str:
    """æœç´¢ç”¨æˆ·è¿‡åŽ»çš„æ—¥è®°å†…å®¹ã€‚å½“ç”¨æˆ·æåˆ°è¿‡åŽ»å†™çš„ä¸œè¥¿ã€æƒ³å›žé¡¾æŸæ®µç»åŽ†ã€æˆ–è€…é—®ã€Œæˆ‘ä¹‹å‰æœ‰æ²¡æœ‰å†™è¿‡å…³äºŽ...ã€æ—¶ä½¿ç”¨è¿™ä¸ªå·¥å…·ã€‚"""
    user_id = config.get("configurable", {}).get("user_id", "")
    if not user_id:
        return "æ— æ³•æœç´¢æ—¥è®°ï¼šç¼ºå°‘ç”¨æˆ·ä¿¡æ¯ã€‚"
    if not query or not query.strip():
        return "è¯·æä¾›æœç´¢å…³é”®è¯ã€‚"

    try:
        results = await vector_service.search_similar_diaries(
            user_id=user_id,
            query=query,
            max_results=max_results,
        )
        if not results:
            return f"æ²¡æœ‰æ‰¾åˆ°ä¸Žã€Œ{query}ã€ç›¸å…³çš„æ—¥è®°è®°å½•ã€‚"

        parts = []
        for i, r in enumerate(results, 1):
            preview = r.get("content_preview", "").strip()
            created = r.get("created_at", "æœªçŸ¥æ—¥æœŸ")
            sim = r.get("similarity", 0)
            parts.append(f"[{i}] æ—¥æœŸ: {created} (ç›¸å…³åº¦: {sim:.2f})\n{preview}")
        return "\n\n---\n\n".join(parts)

    except Exception as e:
        logger.error(f"Diary search tool error: {e}", exc_info=True)
        return "æ—¥è®°æœç´¢å‡ºé”™äº†ï¼Œè¯·ç¨åŽå†è¯•ã€‚"


@tool
async def search_fortune_knowledge(query: str, config: RunnableConfig) -> str:
    """æœç´¢ä¸“ä¸šå‘½ç†çŸ¥è¯†åº“ã€‚å½“ç”¨æˆ·é—®åˆ°å…«å­—ç†è®ºã€äº”è¡Œç›¸ç”Ÿç›¸å…‹ã€åç¥žå«ä¹‰ã€æ ¼å±€è§£è¯»ã€å¡”ç½—ç‰Œæ·±å±‚å«ä¹‰ç­‰å‘½ç†ä¸“ä¸šçŸ¥è¯†æ—¶ä½¿ç”¨ã€‚ä¸åŒäºŽæŸ¥è¯¢ç”¨æˆ·ä¸ªäººå…«å­—ï¼Œè¿™ä¸ªå·¥å…·ç”¨æ¥æ£€ç´¢é€šç”¨çš„å‘½ç†å­¦å‚è€ƒèµ„æ–™ã€‚"""
    user_id = config.get("configurable", {}).get("user_id", "")
    if not query or not query.strip():
        return "è¯·æä¾›è¦æŸ¥è¯¢çš„å‘½ç†çŸ¥è¯†å…³é”®è¯ã€‚"

    try:
        from app.services.knowledge_service import KnowledgeService
        knowledge_svc = KnowledgeService()

        result = await knowledge_svc.get_relevant_knowledge(
            query=query,
            context=f"user_id={user_id}" if user_id else "",
        )

        items = result.get("knowledge", [])
        if not items:
            return f"æ²¡æœ‰æ‰¾åˆ°ä¸Žã€Œ{query}ã€ç›¸å…³çš„å‘½ç†çŸ¥è¯†ã€‚"

        parts = []
        for i, item in enumerate(items[:5], 1):
            content = item.get("content", "").strip()[:300]
            source = item.get("source", "çŸ¥è¯†åº“")
            sim = item.get("similarity", 0)
            parts.append(f"[{i}] ({source}, ç›¸å…³åº¦: {sim:.2f})\n{content}")

        meta = result.get("metadata", {})
        summary = meta.get("source_summary", "")
        header = f"æ‰¾åˆ° {len(items)} æ¡ç›¸å…³çŸ¥è¯†" + (f" | {summary}" if summary else "")
        return header + "\n\n" + "\n\n---\n\n".join(parts)

    except Exception as e:
        logger.error(f"Knowledge search tool error: {e}", exc_info=True)
        return "å‘½ç†çŸ¥è¯†æœç´¢å‡ºé”™äº†ï¼Œè¯·ç¨åŽå†è¯•ã€‚"


def _get_user_birth_date(user_id: str):
    """ä»Ž profiles è¡¨èŽ·å–ç”¨æˆ·ç”Ÿæ—¥ã€‚"""
    from datetime import datetime as dt
    resp = supabase.table("profiles").select("birth_datetime").eq("id", user_id).single().execute()
    if resp.data and resp.data.get("birth_datetime"):
        return dt.fromisoformat(resp.data["birth_datetime"]).date()
    return None


@tool
def query_bazi_info(config: RunnableConfig) -> str:
    """æŸ¥è¯¢ç”¨æˆ·çš„å…«å­—å‘½ç›˜ä¿¡æ¯ã€‚å½“ç”¨æˆ·é—®åˆ°è‡ªå·±çš„å…«å­—ã€æ—¥ä¸»ã€äº”è¡Œã€ä½“è´¨å¼ºå¼±ã€ä»Šæ—¥æµæ—¥è¿åŠ¿ç­‰å‘½ç†ç›¸å…³é—®é¢˜æ—¶ä½¿ç”¨ã€‚"""
    from datetime import date
    user_id = config.get("configurable", {}).get("user_id", "")
    if not user_id:
        return "æ— æ³•æŸ¥è¯¢å…«å­—ï¼šç¼ºå°‘ç”¨æˆ·ä¿¡æ¯ã€‚"
    try:
        birth_date = _get_user_birth_date(user_id)
        if not birth_date:
            return "ä½ è¿˜æ²¡æœ‰è®¾ç½®ç”Ÿæ—¥ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­å¡«å†™å‡ºç”Ÿæ—¥æœŸã€‚"
        bazi = bazi_service.calculate_bazi(birth_date)
        flow = bazi_service.analyze_daily_flow(birth_date, target_date=date.today())
        return (
            f"æ—¥ä¸»: {bazi['day_master']} | ä½“è´¨: {bazi['body_strength']}\n"
            f"å››æŸ±: {bazi['year_pillar']} {bazi['month_pillar']} {bazi['day_pillar']} {bazi['hour_pillar']}\n"
            f"ä»Šæ—¥æµæ—¥: {flow['daily_pillar']['stem']}{flow['daily_pillar']['branch']}\n"
            f"å¤©å¹²å½±å“: {flow['stem_influence']['relation']} â€” {flow['stem_influence']['analysis']}\n"
            f"åœ°æ”¯å½±å“: {flow['branch_influence']['relation']} â€” {flow['branch_influence']['analysis']}\n"
            f"åäºŒé•¿ç”Ÿ: {flow['energy_phase']} | è´µäººåˆ†: {flow['nobleman_score']}"
        )
    except Exception as e:
        logger.error(f"BaZi tool error: {e}", exc_info=True)
        return "å…«å­—æŸ¥è¯¢å‡ºé”™äº†ï¼Œè¯·ç¨åŽå†è¯•ã€‚"


@tool
def query_tarot_info(config: RunnableConfig) -> str:
    """æŸ¥è¯¢ç”¨æˆ·ä»Šæ—¥å¡”ç½—ç‰Œä¿¡æ¯ã€‚å½“ç”¨æˆ·é—®åˆ°ä»Šå¤©çš„å¡”ç½—ç‰Œã€ç‰Œé¢å«ä¹‰ã€æŠ½åˆ°äº†ä»€ä¹ˆç‰Œç­‰å¡”ç½—ç›¸å…³é—®é¢˜æ—¶ä½¿ç”¨ã€‚"""
    from datetime import date
    user_id = config.get("configurable", {}).get("user_id", "")
    if not user_id:
        return "æ— æ³•æŸ¥è¯¢å¡”ç½—ï¼šç¼ºå°‘ç”¨æˆ·ä¿¡æ¯ã€‚"
    try:
        today = date.today()
        reading = tarot_service.draw_daily_card(user_id, today)
        if "error" in reading:
            return f"å¡”ç½—æŸ¥è¯¢å¤±è´¥ï¼š{reading['error']}"
        card = reading.get("card", {})
        orientation = reading.get("orientation", "upright")
        ori_label = "æ­£ä½" if orientation == "upright" else "é€†ä½"
        meaning = card.get("meaning_up") if orientation == "upright" else card.get("meaning_down")
        return (
            f"ä»Šæ—¥å¡”ç½—: {card.get('card_name', 'æœªçŸ¥')} ({ori_label})\n"
            f"ç‰Œä¹‰: {meaning}\n"
            f"æè¿°: {card.get('description', '')}"
        )
    except Exception as e:
        logger.error(f"Tarot tool error: {e}", exc_info=True)
        return "å¡”ç½—æŸ¥è¯¢å‡ºé”™äº†ï¼Œè¯·ç¨åŽå†è¯•ã€‚"


@tool
async def generate_diary(config: RunnableConfig, state: Annotated[dict, InjectedState]) -> str:
    """æ ¹æ®å½“å‰å¯¹è¯å†…å®¹ç”Ÿæˆä¸€ç¯‡æ—¥è®°ã€‚å½“ç”¨æˆ·è¯´ã€Œå¸®æˆ‘ç”Ÿæˆæ—¥è®°ã€ã€Œå†™æ—¥è®°ã€ã€Œè®°å½•ä¸€ä¸‹ä»Šå¤©ã€æˆ–ç‚¹å‡»ç”Ÿæˆæ—¥è®°æŒ‰é’®æ—¶ä½¿ç”¨è¿™ä¸ªå·¥å…·ã€‚ä¸éœ€è¦ä»»ä½•å‚æ•°ï¼Œä¼šè‡ªåŠ¨ä»Žå¯¹è¯åŽ†å²ä¸­æå–å†…å®¹ã€‚"""
    from app.core.genai_service import genai_service
    from app.services.letta_service import letta_service
    from datetime import date, datetime, timezone
    import asyncio
    import json

    user_id = config.get("configurable", {}).get("user_id", "")
    if not user_id:
        return "æ— æ³•ç”Ÿæˆæ—¥è®°ï¼šç¼ºå°‘ç”¨æˆ·ä¿¡æ¯ã€‚"

    # 1. Extract conversation history from graph state
    messages = state.get("messages", [])
    if not messages:
        return "æ— æ³•ç”Ÿæˆæ—¥è®°ï¼šæ²¡æœ‰æ‰¾åˆ°å¯¹è¯å†…å®¹ã€‚è¯·å…ˆå’Œæˆ‘èŠèŠä»Šå¤©å‘ç”Ÿäº†ä»€ä¹ˆã€‚"

    # Build conversation text from human messages
    conversation_parts = []
    for msg in messages:
        if hasattr(msg, "content") and isinstance(msg.content, str):
            if hasattr(msg, "type"):
                if msg.type == "human":
                    conversation_parts.append(f"ç”¨æˆ·: {msg.content}")
                elif msg.type == "ai" and msg.content and not getattr(msg, "tool_calls", None):
                    conversation_parts.append(f"åŠ©æ‰‹: {msg.content}")

    conversation_text = "\n".join(conversation_parts[-20:])  # Last 20 messages max

    if not conversation_text.strip():
        return "æ— æ³•ç”Ÿæˆæ—¥è®°ï¼šå¯¹è¯å†…å®¹ä¸ºç©ºã€‚è¯·å…ˆå’Œæˆ‘èŠèŠä»Šå¤©å‘ç”Ÿäº†ä»€ä¹ˆã€‚"

    # 2. Summarize conversation into diary content
    summary_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆä¸€ç¯‡ç®€çŸ­çš„æ—¥è®°ï¼ˆ150-300å­—ï¼‰ã€‚
è¦æ±‚ï¼š
- ç”¨ç¬¬ä¸€äººç§°ä¹¦å†™
- æå–å¯¹è¯ä¸­çš„å…³é”®äº‹ä»¶ã€æƒ…ç»ªå’Œæ„Ÿæ‚Ÿ
- è¯­è°ƒè‡ªç„¶ã€çœŸå®žï¼Œåƒæ˜¯è‡ªå·±å†™ç»™è‡ªå·±çš„è®°å½•
- ä¸è¦æåŠ"åŠ©æ‰‹"æˆ–"AI"çš„å­˜åœ¨

ã€å¯¹è¯å†…å®¹ã€‘
{conversation_text}

è¯·ç›´æŽ¥è¾“å‡ºæ—¥è®°å†…å®¹ï¼Œä¸è¦åŠ æ ‡é¢˜æˆ–é¢å¤–è¯´æ˜Žã€‚"""

    try:
        diary_content = await genai_service.generate_text(summary_prompt)
    except Exception as e:
        logger.error(f"Diary content generation failed: {e}", exc_info=True)
        return "æ—¥è®°å†…å®¹ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åŽå†è¯•ã€‚"

    # 3. Get today's fortune for instant feedback
    today = date.today()
    battery_fortune = None
    try:
        resp = supabase.table("daily_fortune_details").select("battery_fortune").eq("user_id", user_id).eq("fortune_date", today.isoformat()).single().execute()
        if resp.data:
            battery_fortune = resp.data.get("battery_fortune")
    except Exception:
        pass

    # 4. Generate instant feedback
    if battery_fortune:
        overall = battery_fortune.get("overall", {})
        feedback_prompt = f"""ä½œä¸ºä¸€ä½å……æ»¡æ™ºæ…§å’ŒåŒç†å¿ƒçš„æœ‹å‹ï¼Œè¯·é˜…è¯»ä»¥ä¸‹å†…å®¹å¹¶ç»™å‡ºæ¸©æš–åé¦ˆã€‚

ã€æœ‹å‹ä»Šå¤©çš„æ—¥è®°ã€‘
{diary_content}

ã€ä»Šæ—¥è¿åŠ¿å‚è€ƒã€‘
ä»Šæ—¥æ•´ä½“: {overall.get('daily_management', '')}
é¡ºæ‰‹çš„äº‹: {overall.get('today_actions', '')}

è¯·ç»“åˆæ—¥è®°å†…å®¹å’Œè¿åŠ¿ä¿¡æ¯ï¼Œç»™å‡º50-100å­—çš„æ¸©æš–ã€é¼“åŠ±çš„åé¦ˆã€‚"""
    else:
        feedback_prompt = f"""ä½œä¸ºä¸€ä½å……æ»¡æ™ºæ…§å’ŒåŒç†å¿ƒçš„æœ‹å‹ï¼Œè¯·é˜…è¯»ä»¥ä¸‹æ—¥è®°å¹¶ç»™å‡ºæ¸©æš–åé¦ˆã€‚

ã€æœ‹å‹ä»Šå¤©çš„æ—¥è®°ã€‘
{diary_content}

è¯·ç»™å‡º50-100å­—çš„æ¸©æš–ã€é¼“åŠ±çš„åé¦ˆã€‚è¯­è°ƒäº²åˆ‡è‡ªç„¶ã€‚"""

    try:
        instant_feedback = await genai_service.generate_text(feedback_prompt)
    except Exception:
        instant_feedback = ""

    # 5. Generate embedding
    try:
        embedding = await genai_service.generate_embedding(diary_content)
    except Exception:
        embedding = None

    # 6. Save to database
    diary_data = {
        "user_id": user_id,
        "content": diary_content,
        "emotion_tags": [],
        "instant_feedback": instant_feedback,
        "embedding": embedding,
    }

    try:
        response = supabase.table("diary_entries").insert(diary_data).execute()
        if not response.data:
            return "æ—¥è®°ä¿å­˜å¤±è´¥ï¼Œè¯·ç¨åŽå†è¯•ã€‚"
        created_entry = response.data[0]
        logger.info(f"Diary generated via agent tool - ID: {created_entry['id']}, User: {user_id}")
    except Exception as e:
        logger.error(f"Diary save failed: {e}", exc_info=True)
        return "æ—¥è®°ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ï¼Œè¯·ç¨åŽå†è¯•ã€‚"

    # 7. Ingest to Letta (background)
    try:
        diary_date = created_entry["created_at"][:10] if created_entry.get("created_at") else None

        async def _ingest():
            try:
                await letta_service.ingest_diary(user_id=user_id, diary_text=diary_content, diary_date=diary_date)
            except Exception:
                pass

        asyncio.create_task(_ingest())
    except Exception:
        pass

    # 8. Return result as structured JSON for frontend rendering
    result = {
        "diary_id": str(created_entry["id"]),
        "content": diary_content,
        "insight": instant_feedback,
        "created_at": created_entry.get("created_at", ""),
    }
    return json.dumps(result, ensure_ascii=False)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Agent service
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class ChatAgentService:
    """
    Manages the LangGraph reactive agent lifecycle.

    - One shared connection pool + checkpointer
    - One compiled graph (stateless â€” state lives in the checkpointer)
    - Per-request: pass thread_id + user_id through config
    """

    def __init__(self):
        self.pool: Optional[AsyncConnectionPool] = None
        self.graph = None
        self._initialized = False

    # â”€â”€ lifecycle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def initialize(self):
        """Call once at app startup."""
        if self._initialized:
            return

        if not SUPABASE_DB_URI:
            logger.warning("âš ï¸  SUPABASE_DB_URI not set â€” agent disabled")
            return
        if not GOOGLE_API_KEY:
            logger.warning("âš ï¸  GOOGLE_API_KEY not set â€” agent disabled")
            return

        try:
            # 1. Postgres connection pool
            self.pool = AsyncConnectionPool(
                conninfo=SUPABASE_DB_URI,
                max_size=20,
                min_size=2,
                open=False,
                kwargs={"prepare_threshold": None},
            )
            await self.pool.open()
            logger.info("âœ… DB pool opened")

            # 2. Run checkpointer migrations once
            async with self.pool.connection() as conn:
                await conn.set_autocommit(True)
                checkpointer = AsyncPostgresSaver(conn)
                await checkpointer.setup()
            logger.info("âœ… Checkpointer tables ready")

            # 3. Build the graph (once â€” it's reusable)
            model = ChatGoogleGenerativeAI(
                model=DEFAULT_CHAT_MODEL,
                google_api_key=GOOGLE_API_KEY,
                temperature=0.7,
            )

            # We create the checkpointer from the pool for the compiled graph.
            # create_react_agent compiles a graph with the ReAct loop baked in.
            graph_checkpointer = AsyncPostgresSaver(self.pool)
            self.graph = create_react_agent(
                model=model,
                tools=[search_diaries, search_fortune_knowledge, query_bazi_info, query_tarot_info, generate_diary],
                checkpointer=graph_checkpointer,
            )

            self._initialized = True
            logger.info(f"ðŸŽ‰ Chat agent ready  (model={DEFAULT_CHAT_MODEL})")

        except Exception as e:
            logger.error(f"âŒ Agent init failed: {e}", exc_info=True)
            if self.pool:
                try:
                    await self.pool.close()
                except Exception:
                    pass
                self.pool = None
            self._initialized = False

    async def shutdown(self):
        if self.pool:
            await self.pool.close()
            logger.info("Agent pool closed")

    # â”€â”€ public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def chat(
        self,
        conversation_id: str,
        user_id: str,
        message: str,
        user_profile: str = "",
        today_fortune: dict | None = None,
        language: str = "zh-CN",
    ) -> str:
        """
        Send a message and get a full response.

        The system prompt (with fortune + profile context) is prepended
        as the first message only when starting a new conversation.
        LangGraph's checkpointer handles history automatically.
        """
        if not self._initialized or not self.graph:
            raise RuntimeError("Agent not initialized")

        config = {
            "configurable": {
                "thread_id": conversation_id,
                "user_id": user_id,
            }
        }

        # Build input messages
        messages = []

        # Check if this is a fresh conversation (no checkpoint yet)
        state = await self.graph.aget_state(config)
        if not state or not state.values.get("messages"):
            # First turn â€” inject system prompt with full fortune context
            fortune_ctx = await load_fortune_context(user_id)
            system_prompt = build_system_prompt(
                user_profile=user_profile,
                today_fortune=fortune_ctx["today_fortune"] or today_fortune,
                daily_bazi=fortune_ctx["daily_bazi"],
                daily_tarot=fortune_ctx["daily_tarot"],
                recent_recharges_block=fortune_ctx["recent_recharges_block"],
                yesterday_diary_block=fortune_ctx["yesterday_diary_block"],
                language=language,
            )
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": message})

        result = await self.graph.ainvoke({"messages": messages}, config)

        # Extract the last AI message
        if result and "messages" in result and result["messages"]:
            last = result["messages"][-1]
            if hasattr(last, "content"):
                content = last.content
                if isinstance(content, list):
                    return "".join(
                        item.get("text", "") if isinstance(item, dict) else str(item)
                        for item in content
                    )
                return content
        return "æŠ±æ­‰ï¼Œæˆ‘çŽ°åœ¨æ— æ³•å›žå¤ã€‚"

    async def chat_stream(
        self,
        conversation_id: str,
        user_id: str,
        message: str,
        user_profile: str = "",
        today_fortune: dict | None = None,
        language: str = "zh-CN",
    ):
        """
        Streaming version â€” yields token chunks as they arrive.
        """
        if not self._initialized or not self.graph:
            raise RuntimeError("Agent not initialized")

        config = {
            "configurable": {
                "thread_id": conversation_id,
                "user_id": user_id,
            }
        }

        messages = []
        state = await self.graph.aget_state(config)
        if not state or not state.values.get("messages"):
            fortune_ctx = await load_fortune_context(user_id)
            system_prompt = build_system_prompt(
                user_profile=user_profile,
                today_fortune=fortune_ctx["today_fortune"] or today_fortune,
                daily_bazi=fortune_ctx["daily_bazi"],
                daily_tarot=fortune_ctx["daily_tarot"],
                recent_recharges_block=fortune_ctx["recent_recharges_block"],
                yesterday_diary_block=fortune_ctx["yesterday_diary_block"],
                language=language,
            )
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": message})

        async for chunk, metadata in self.graph.astream(
            {"messages": messages},
            config,
            stream_mode="messages",
        ):
            if hasattr(chunk, "content") and chunk.content:
                content = chunk.content
                if isinstance(content, list):
                    yield "".join(
                        item.get("text", "") if isinstance(item, dict) else str(item)
                        for item in content
                    )
                else:
                    yield content


# â”€â”€ Singleton â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

chat_agent = ChatAgentService()


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# AG-UI / CopilotKit graph (with thinking_buffer for frontend)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Tool sets per chat type
FORTUNE_TOOLS = [search_diaries, search_fortune_knowledge, query_bazi_info, query_tarot_info]
DIARY_TOOLS = [search_diaries, generate_diary]
ALL_TOOLS = [search_diaries, search_fortune_knowledge, query_bazi_info, query_tarot_info, generate_diary]


class AgentState(CopilotKitState):
    """CopilotKitState + thinking buffer for frontend display."""
    thinking_buffer: str


def _extract_thinking(content) -> str:
    """Extract thinking blocks from AIMessage.content (list format when include_thoughts=True)."""
    if not isinstance(content, list):
        return ""
    return "\n\n".join(
        block.get("thinking", "")
        for block in content
        if isinstance(block, dict) and block.get("type") == "thinking" and block.get("thinking")
    )


def _build_llm() -> ChatGoogleGenerativeAI:
    kwargs = dict(
        model=DEFAULT_CHAT_MODEL,
        google_api_key=GOOGLE_API_KEY,
        temperature=1.0,
    )
    if THINKING_ENABLED:
        kwargs["thinking_level"] = THINKING_LEVEL.lower()
        kwargs["include_thoughts"] = True
    return ChatGoogleGenerativeAI(**kwargs)


async def _agent_node(state: AgentState, config: RunnableConfig):
    from app.services.letta_service import letta_service

    user_id: str = config.get("configurable", {}).get("user_id", "")
    chat_type: str = config.get("configurable", {}).get("chat_type", "fortune")
    language: str = config.get("configurable", {}).get("language", "zh-CN")

    user_profile = ""
    if user_id:
        try:
            user_profile = await letta_service.get_user_profile(user_id)
        except Exception as exc:
            logger.warning("Profile fetch failed for %s: %s", user_id, exc)

    # Load fortune context from DB
    fortune_ctx = await load_fortune_context(user_id)

    # Select prompt and tools based on chat_type
    if chat_type == "diary":
        system_prompt = build_diary_system_prompt(
            user_profile=user_profile,
            today_fortune=fortune_ctx["today_fortune"],
            language=language,
        )
        tools = DIARY_TOOLS
    else:
        system_prompt = build_system_prompt(
            user_profile=user_profile,
            today_fortune=fortune_ctx["today_fortune"],
            daily_bazi=fortune_ctx["daily_bazi"],
            daily_tarot=fortune_ctx["daily_tarot"],
            recent_recharges_block=fortune_ctx["recent_recharges_block"],
            yesterday_diary_block=fortune_ctx["yesterday_diary_block"],
            language=language,
        )
        tools = FORTUNE_TOOLS

    llm = _build_llm().bind_tools(tools)
    messages = [SystemMessage(content=system_prompt), *state["messages"]]

    # Stream to capture thinking chunks in real-time
    prev = state.get("thinking_buffer", "") or ""
    thinking_buffer = prev
    full_message = None

    async for chunk in llm.astream(messages, config=config):
        if full_message is None:
            full_message = chunk
        else:
            full_message = full_message + chunk

        # Extract thinking from this chunk and emit immediately
        new_thinking = _extract_thinking(chunk.content)
        if new_thinking:
            separator = "\n\n---\n\n" if thinking_buffer else ""
            thinking_buffer = thinking_buffer + separator + new_thinking
            await copilotkit_emit_state(config, {"thinking_buffer": thinking_buffer})
            logger.info("ðŸ§  thinking streamed: +%d chars (total %d)", len(new_thinking), len(thinking_buffer))

    ai_message = AIMessage(
        content=full_message.content,
        tool_calls=full_message.tool_calls,
        additional_kwargs=full_message.additional_kwargs,
    )

    logger.info("ðŸ§  thinking final: %d chars, blocks: %s",
                len(thinking_buffer),
                [b.get("type") for b in ai_message.content if isinstance(b, dict)] if isinstance(ai_message.content, list) else "str")

    return {"messages": [ai_message], "thinking_buffer": thinking_buffer}


def _should_continue(state: AgentState) -> Literal["tools", "__end__"]:
    last = state["messages"][-1]
    if isinstance(last, AIMessage) and last.tool_calls:
        return "tools"
    return END


_agui_compiled = None


def build_agui_graph():
    global _agui_compiled
    if _agui_compiled is not None:
        return _agui_compiled

    if not GOOGLE_API_KEY:
        raise RuntimeError("GOOGLE_API_KEY not set")

    wf = StateGraph(AgentState)
    wf.add_node("agent", _agent_node)
    wf.add_node("tools", ToolNode(ALL_TOOLS))
    wf.set_entry_point("agent")
    wf.add_conditional_edges("agent", _should_continue, {"tools": "tools", END: END})
    wf.add_edge("tools", "agent")

    _agui_compiled = wf.compile(checkpointer=InMemorySaver())
    logger.info("AG-UI graph compiled (model=%s, tools=%s, thinking=%s)",
                DEFAULT_CHAT_MODEL, [t.name for t in ALL_TOOLS], THINKING_ENABLED)
    return _agui_compiled
