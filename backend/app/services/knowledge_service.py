from typing import List, Dict, Any, Optional
import logging
from .vector_service import VectorService
from .google_search_service import GoogleSearchService


class KnowledgeService:
    """çŸ¥è¯†æ£€ç´¢å’Œç®¡ç†æœåŠ¡ - é›†æˆåŠ¨æ€æƒé‡ä¸æ™ºèƒ½æœç´¢"""

    def __init__(self):
        self.similarity_threshold = 0.7
        self.max_results = 5
        self.vector_service = VectorService()
        self.google_search = GoogleSearchService()

    def _should_trigger_web_search(self, knowledge_results: List[Dict], query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘è”ç½‘æœç´¢"""
        if len(knowledge_results) < 3:
            high_quality_count = len([k for k in knowledge_results if k.get('similarity', 0) > 0.6])
            if high_quality_count == 0:
                return True
        return False

    async def _google_search_knowledge(self, query: str, context: str = "") -> List[Dict[str, Any]]:
        """æ‰§è¡Œ Google Search Grounding è·å–ç›¸å…³çŸ¥è¯†"""
        try:
            logging.info(f"è§¦å‘Googleæœç´¢: {query}")
            search_response = await self.google_search.search_with_grounding(query, context)
            formatted_results = self.google_search.format_search_results(search_response)
            if formatted_results:
                logging.info(f"Googleæœç´¢æˆåŠŸï¼Œè·å¾— {len(formatted_results)} æ¡ç»“æœ")
                return formatted_results
            logging.warning("Googleæœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ")
            return []
        except Exception as e:
            logging.error(f"Googleæœç´¢å¤±è´¥: {str(e)}")
            return []

    def _classify_knowledge_sources(self, results: List[Dict]) -> Dict[str, List[Dict]]:
        """å¯¹çŸ¥è¯†æ¥æºè¿›è¡Œåˆ†ç±»"""
        classified = {"local": [], "google": [], "web": []}
        for result in results:
            if result.get("type") == "google_search":
                classified["google"].append(result)
            elif result.get("is_web_result", False):
                classified["web"].append(result)
            else:
                classified["local"].append(result)
        return classified

    def _generate_source_summary(self, classified_sources: Dict[str, List[Dict]]) -> str:
        """ç”Ÿæˆæ•°æ®æºæ‘˜è¦"""
        labels = [
            ("local", "ğŸ“š æœ¬åœ°çŸ¥è¯†åº“"),
            ("google", "ğŸ” Googleæœç´¢"),
            ("web", "ğŸŒ ç½‘ç»œèµ„æº"),
        ]
        parts = [f"{label} ({len(classified_sources[key])}æ¡)"
                 for key, label in labels if classified_sources[key]]
        return f"**æ•°æ®æº**: {' + '.join(parts)}" if parts else "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†èµ„æº"

    def _analyze_result_quality(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†æç»“æœè´¨é‡"""
        if not results:
            return {"level": "æ— æ•°æ®", "description": "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"}

        similarities = [r.get('similarity', 0) for r in results]
        avg_similarity = sum(similarities) / len(similarities)
        high_quality_count = len([s for s in similarities if s > 0.7])

        if avg_similarity >= 0.8:
            level, description = "é«˜è´¨é‡", f"å¹³å‡ç›¸å…³åº¦ {avg_similarity:.1f}ï¼ŒåŒ…å« {high_quality_count} æ¡é«˜è´¨é‡ä¿¡æ¯"
        elif avg_similarity >= 0.6:
            level, description = "ä¸­ç­‰è´¨é‡", f"å¹³å‡ç›¸å…³åº¦ {avg_similarity:.1f}ï¼Œå»ºè®®ç»“åˆä¸“ä¸šå’¨è¯¢"
        else:
            level, description = "è¾ƒä½è´¨é‡", f"å¹³å‡ç›¸å…³åº¦ {avg_similarity:.1f}ï¼Œå»ºè®®å¯»æ‰¾æ›´ä¸“ä¸šçš„ä¿¡æ¯æº"

        return {
            "level": level, "description": description,
            "avg_similarity": round(avg_similarity, 2),
            "high_quality_count": high_quality_count,
            "total_count": len(results),
        }

    # â”€â”€ åŠ¨æ€æƒé‡ï¼ˆæ¥è‡ªV2ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _apply_dynamic_weighting(self, knowledge_items: List[Dict], query: str) -> List[Dict]:
        """æ ¹æ®æŸ¥è¯¢ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ç›¸ä¼¼åº¦æƒé‡"""
        boost_rules = [
            (lambda q, c: any(w in q for w in ["ä»Šå¤©", "ä»Šæ—¥", "å½“æ—¥"])
                          and any(t in c for t in ["å½“æ—¥", "ä»Šæ—¥è¿åŠ¿", "æ—¥è¿"]),
             0.15, "æ—¶æ•ˆæ€§åŒ¹é…"),
            (lambda q, c: "ä¸™ç«" in q and "ä¸™ç«" in c and "æ—¥ä¸»" in c,
             0.10, "ä¸“ä¸šæœ¯è¯­åŒ¹é…"),
            (lambda q, c: any(w in q for w in ["èŒä¸š", "å·¥ä½œ", "äº‹ä¸š"])
                          and any(w in c for w in ["èŒä¸š", "äº‹ä¸š", "å·¥ä½œ"]),
             0.12, "åº”ç”¨åœºæ™¯åŒ¹é…"),
        ]
        for item in knowledge_items:
            content = item.get('content', '')
            base = item.get('similarity', 0)
            for check, boost, reason in boost_rules:
                if check(query, content):
                    item['similarity'] = min(base + boost, 1.0)
                    item['boost_reason'] = reason
                    break
        return sorted(knowledge_items, key=lambda x: x.get('similarity', 0), reverse=True)

    # â”€â”€ æ¶ˆæ­§ä¹‰ï¼ˆæ¥è‡ªV2ï¼Œé»˜è®¤ç¦ç”¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def disambiguate_query(query: str) -> str:
        """ä¸“ä¸šæœ¯è¯­æ¶ˆæ­§å¤„ç†"""
        if "ä¸™ç«æ—¥ä¸»" in query and "å¤©å¹²ä¸™ç«" in query:
            return "ä¸™ç«æ—¥ä¸»åœ¨å¤©å¹²ä¸™ç«æ—¥çš„è¿åŠ¿åˆ†æï¼Œé‡ç‚¹å…³æ³¨åŒå¹²é‡å¤çš„å½±å“å’Œèƒ½é‡å åŠ æ•ˆåº”"
        if "é€†ä½" in query and "æ„Ÿæƒ…" in query:
            return f"{query}ï¼Œé‡ç‚¹åˆ†æé€†ä½çŠ¶æ€ä¸‹çš„æ„Ÿæƒ…èƒ½é‡å’ŒæŒ‘æˆ˜"
        if "æ ¼å±€" in query and any(w in query for w in ["é€‚åˆ", "èŒä¸š", "å·¥ä½œ"]):
            return f"{query}ï¼Œé‡ç‚¹ä»å…«å­—æ ¼å±€ç‰¹ç‚¹åˆ†æé€‚åˆçš„èŒä¸šæ–¹å‘å’Œå‘å±•å»ºè®®"
        return query

    # â”€â”€ æ ¸å¿ƒæ£€ç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def get_relevant_knowledge(
        self, query: str, context: str = "",
        include_web_search: bool = True,
        enable_disambiguation: bool = False,
        enable_dynamic_weight: bool = True,
    ) -> Dict[str, Any]:
        """è·å–ç›¸å…³çŸ¥è¯†ï¼Œé›†æˆåŠ¨æ€æƒé‡ä¸æ™ºèƒ½è”ç½‘æœç´¢"""
        try:
            processed_query = self.disambiguate_query(query) if enable_disambiguation else query

            # 1. æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
            local_results = await self.vector_service.search_similar_content(
                query=processed_query,
                threshold=self.similarity_threshold,
                max_results=self.max_results,
            )
            all_results = local_results.copy()
            web_search_triggered = False

            # 2. æ™ºèƒ½è”ç½‘æœç´¢
            if include_web_search and self._should_trigger_web_search(local_results, processed_query):
                logging.info(f"è§¦å‘æ™ºèƒ½è”ç½‘æœç´¢ - æœ¬åœ°ç»“æœæ•°é‡: {len(local_results)}")
                web_search_triggered = True
                all_results.extend(await self._google_search_knowledge(processed_query, context))

            # 3. åŠ¨æ€æƒé‡è°ƒæ•´
            if all_results and enable_dynamic_weight:
                all_results = self._apply_dynamic_weighting(all_results, processed_query)

            # 4. åˆ†ç±»ä¸æ‘˜è¦
            classified = self._classify_knowledge_sources(all_results)
            source_summary = self._generate_source_summary(classified)

            result = {
                "knowledge": all_results,
                "metadata": {
                    "total_results": len(all_results),
                    "local_count": len(classified["local"]),
                    "google_count": len(classified["google"]),
                    "web_count": len(classified["web"]),
                    "web_search_triggered": web_search_triggered,
                    "search_trigger_reason": "æ™ºèƒ½æ£€æµ‹åˆ°éœ€è¦è¡¥å……ä¿¡æ¯" if web_search_triggered else "æœ¬åœ°çŸ¥è¯†å……è¶³",
                    "source_summary": source_summary,
                    "quality_info": self._analyze_result_quality(all_results),
                    "disambiguation_applied": enable_disambiguation,
                    "dynamic_weight_applied": enable_dynamic_weight,
                    "original_query": query,
                    "processed_query": processed_query,
                },
            }
            logging.info(f"çŸ¥è¯†æ£€ç´¢å®Œæˆ - æ€»è®¡: {len(all_results)} æ¡ï¼Œè”ç½‘æœç´¢: {'æ˜¯' if web_search_triggered else 'å¦'}")
            return result

        except Exception as e:
            logging.error(f"çŸ¥è¯†æ£€ç´¢å¤±è´¥: {str(e)}")
            return {
                "knowledge": [],
                "metadata": {"total_results": 0, "error": str(e), "source_summary": "âš ï¸ çŸ¥è¯†æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"},
            }

    # â”€â”€ Promptå¢å¼ºï¼ˆæ¥è‡ªV2ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def enhance_prompt_with_knowledge(
        self, base_prompt: str, context_query: str,
        categories: Optional[List[str]] = None,
        enable_disambiguation: bool = False,
        enable_dynamic_weight: bool = True,
    ) -> str:
        """ä½¿ç”¨ä¸“ä¸šçŸ¥è¯†å¢å¼ºprompt"""
        try:
            knowledge_result = await self.get_relevant_knowledge(
                query=context_query, context=base_prompt,
                enable_disambiguation=enable_disambiguation,
                enable_dynamic_weight=enable_dynamic_weight,
            )
            items = knowledge_result["knowledge"]
            if not items:
                return base_prompt

            knowledge_text = ""
            for i, k in enumerate(items[:3], 1):
                sim = k.get('similarity', 0)
                content = k.get('content', '')[:200]
                source = k.get('source', 'ä¸“ä¸šçŸ¥è¯†åº“')
                knowledge_text += f"\nçŸ¥è¯†{i} (ç›¸å…³åº¦:{sim:.2f}ï¼Œæ¥æº:{source}):\n{content}\n"

            return f"""{base_prompt}

ã€ä¸“ä¸šçŸ¥è¯†å‚è€ƒã€‘:
{knowledge_text}

ã€ç”Ÿæˆè¦æ±‚ã€‘:
- è¯·åŸºäºä»¥ä¸Šä¸“ä¸šçŸ¥è¯†ç”Ÿæˆå›ç­”ï¼Œç¡®ä¿å†…å®¹çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
- ç»“åˆç”¨æˆ·çš„å…·ä½“æƒ…å†µï¼ˆå…«å­—ã€æ—¥æœŸç­‰ï¼‰ç»™å‡ºä¸ªæ€§åŒ–å»ºè®®
- å¦‚æœçŸ¥è¯†ä¸­æœ‰ç›¸å†²çªçš„è§‚ç‚¹ï¼Œè¯·ä»¥ç›¸å…³åº¦æœ€é«˜çš„ä¸ºå‡†
- ä¿æŒæ¸©æš–é¼“åŠ±çš„è¯­è°ƒï¼Œé¿å…è¿‡äºä¸¥è‚ƒæˆ–è´Ÿé¢çš„è¡¨è¾¾
- å¦‚æœä¸“ä¸šçŸ¥è¯†ä¸è¶³ä»¥æ”¯æ’‘å›ç­”ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶å»ºè®®å¯»æ±‚æ›´ä¸“ä¸šçš„å’¨è¯¢

è¯·å¼€å§‹ç”Ÿæˆä¸“ä¸šçš„è¿åŠ¿è§£è¯»ï¼š
"""
        except Exception as e:
            logging.error(f"Promptå¢å¼ºå¤±è´¥: {e}")
            return base_prompt

    # â”€â”€ ç‰¹å®šç±»åˆ«æœç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def search_specific_knowledge(self, query: str, category: Optional[str] = None, force_web_search: bool = False) -> List[Dict[str, Any]]:
        """æœç´¢ç‰¹å®šç±»åˆ«çš„çŸ¥è¯†"""
        try:
            enhanced_query = f"{query} {category}" if category else query
            results = await self.vector_service.search_similar_content(
                query=enhanced_query,
                threshold=self.similarity_threshold,
                max_results=self.max_results,
                category_filter=category,
            )
            if force_web_search or self._should_trigger_web_search(results, query):
                results.extend(await self._google_search_knowledge(enhanced_query))
            return results
        except Exception as e:
            logging.error(f"ç‰¹å®šçŸ¥è¯†æœç´¢å¤±è´¥: {str(e)}")
            return []

    # â”€â”€ æ•°æ®åº“æ“ä½œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def update_knowledge_vectors(self, batch_size: int = 10) -> int:
        """æ‰¹é‡ä¸ºçŸ¥è¯†åº“æ¡ç›®ç”Ÿæˆå‘é‡åµŒå…¥"""
        try:
            from ..core.db import supabase
            from .genai_service import genai_service

            response = supabase.table("fortune_knowledge") \
                .select("id, content, title") \
                .is_("embedding", "null") \
                .limit(batch_size) \
                .execute()

            if not response.data:
                logging.info("æ‰€æœ‰çŸ¥è¯†æ¡ç›®éƒ½å·²æœ‰å‘é‡åµŒå…¥")
                return 0

            updated_count = 0
            for item in response.data:
                try:
                    embedding = await genai_service.generate_embedding(item['content'])
                    update_resp = supabase.table("fortune_knowledge") \
                        .update({"embedding": embedding}) \
                        .eq("id", item['id']) \
                        .execute()
                    if update_resp.data:
                        updated_count += 1
                except Exception as e:
                    logging.error(f"å‘é‡ç”Ÿæˆå¤±è´¥ ID {item['id']}: {str(e)}")
                    continue

            logging.info(f"æ‰¹æ¬¡å®Œæˆ: {updated_count}/{len(response.data)} æ¡è®°å½•æ›´æ–°æˆåŠŸ")
            return updated_count
        except Exception as e:
            logging.error(f"æ‰¹é‡å‘é‡æ›´æ–°å¤±è´¥: {str(e)}")
            return 0

    async def get_usage_stats(self, days: int = 30) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            from ..core.db import supabase
            total_response = supabase.table("fortune_knowledge").select("id", count="exact").execute()
            total_knowledge = total_response.count or 0
            vectorized_response = supabase.table("fortune_knowledge") \
                .select("id", count="exact") \
                .not_.is_("embedding", "null") \
                .execute()
            vectorized_count = vectorized_response.count or 0
            return {
                "total_knowledge": total_knowledge,
                "vectorized_count": vectorized_count,
                "pending_vectorization": total_knowledge - vectorized_count,
                "vectorization_progress": round((vectorized_count / total_knowledge * 100) if total_knowledge > 0 else 0, 2),
            }
        except Exception as e:
            logging.error(f"è·å–ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}

    async def get_knowledge_by_category(self, category: str, limit: int = 100) -> List[Dict[str, Any]]:
        """æ ¹æ®åˆ†ç±»è·å–çŸ¥è¯†æ¡ç›®"""
        try:
            from ..core.db import supabase
            response = supabase.table("fortune_knowledge") \
                .select("*") \
                .eq("category", category) \
                .limit(limit) \
                .execute()
            return response.data or []
        except Exception as e:
            logging.error(f"è·å–åˆ†ç±»çŸ¥è¯†å¤±è´¥: {str(e)}")
            return []

    async def add_knowledge_item(self, title: str, content: str, category: str) -> bool:
        """æ·»åŠ æ–°çš„çŸ¥è¯†æ¡ç›®åˆ°æ•°æ®åº“"""
        try:
            from ..core.db import supabase
            data = {
                "title": title,
                "content": content,
                "category": category,
                "embedding": await self.vector_service.generate_embedding(content),
            }
            response = supabase.table("fortune_knowledge").insert(data).execute()
            return bool(response.data)
        except Exception as e:
            logging.error(f"æ·»åŠ çŸ¥è¯†å¤±è´¥: {str(e)}")
            return False

    async def refresh_knowledge_cache(self) -> bool:
        """åˆ·æ–°çŸ¥è¯†ç¼“å­˜"""
        try:
            logging.info("çŸ¥è¯†ç¼“å­˜åˆ·æ–°å®Œæˆ")
            return True
        except Exception as e:
            logging.error(f"åˆ·æ–°çŸ¥è¯†ç¼“å­˜å¤±è´¥: {str(e)}")
            return False

    def get_search_status(self) -> Dict[str, Any]:
        """è·å–æœç´¢æœåŠ¡çŠ¶æ€"""
        return {
            "local_service": "å¯ç”¨",
            "google_search": "æ— é™åˆ¶å¯ç”¨",
            "web_search": "æ™ºèƒ½è§¦å‘",
            "status": "æ­£å¸¸",
            "limitations": "ä¾èµ–APIè‡ªèº«é…é¢",
            "trigger_conditions": {
                "local_results_count": "< 3æ¡",
                "high_quality_count": "= 0æ¡",
                "similarity_threshold": "> 0.6",
            },
        }
